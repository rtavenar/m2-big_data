{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation de Hadoop\n",
        "\n",
        "#### Question #1\n",
        "\n",
        "A l'aide de la commande [`wget`](https://www.gnu.org/software/wget/manual/wget.html) télécharger dans le répertoire courant l'archive compressée de la dernière version des sources d'[Hadoop](https://hadoop.apache.org/)"
      ],
      "metadata": {
        "id": "Y-PtZLsYdZD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg_8iPX5c_Vu"
      },
      "outputs": [],
      "source": [
        "!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #2\n",
        "\n",
        "En utilisant la commande [`tar`](https://www.gnu.org/software/tar/manual/tar.html) décompresser et extraire le contenu de cette archive dans le répertoire courant."
      ],
      "metadata": {
        "id": "iGxCU2CWfLMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "8tyFMH16g_RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mise en place de Java\n",
        "L'utilisation de Hadoop nécessite d'avoir accès à Java. Une installation de Java existe par défaut sur les machines virtuelles fournies par Google Colab mais il est nécessaire d'indiquer le répertoire d'installation de Java dans une varible d'environnement `JAVA_HOME`.\n",
        "\n",
        "#### Question #3\n",
        "Chercher sur le web une commande Unix permettant d'obtenir le répertoire d'installation de Java sur une machine Linux."
      ],
      "metadata": {
        "id": "EvhEl-j-hLiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "sYhXalJwilwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #4\n",
        "A l'aide du module `os` de Python, affecter à la variable d'environnement `JAVA_HOME` la valeur du chemin trouvé à la question précédente"
      ],
      "metadata": {
        "id": "atfWpmEfkCQz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2I8yUhfikkyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test de Hadoop et utilisation de HDFS\n",
        "\n",
        "#### Question #5\n",
        "Tester le fonctionnement du binaire exécutable `hadoop` en affichant son aide (option `--help`)\n",
        "\n",
        "#### Question #6\n",
        "A l'aide de la commande `wget` télécharger dans le dossier courant les données météo journalières correspondant à la ville de Buffalo pour les années 2020, 2021 et 2022 (trois fichiers) depuis le site du [National Center for Environmental Information](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00546)"
      ],
      "metadata": {
        "id": "XqRjZXrHkt3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "Mkxf97r1m_-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #7\n",
        "\n",
        "En utilisant Hadoop, créer sur le HDFS un répertoire nommé `/hdfs`"
      ],
      "metadata": {
        "id": "p-OsM9BcnUC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "8am8MbdOnfl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #8\n",
        "En utilisant Hadoop, copier les fichiers textes (extension `.txt`) depuis le répertoire courant du système local vers le répertoire `/hdfs` du HDFS."
      ],
      "metadata": {
        "id": "cMB6exlqnkeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "StInyQywpNvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #9\n",
        "En utilisant Hadoop, créer un sous répertoire `input` dans le répertoire `/hdfs` du HDFS puis déplacer les fichiers textes depuis `/hdfs` dans ce nouveau répertoire."
      ],
      "metadata": {
        "id": "CZJIozkupUxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "_Lau0u4Vpqm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "wwBTIFrUqPUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce avec Hadoop Streaming\n",
        "Le but de cette partie est d'utiliser MapReduce sur Hadoop pour obtenir la valeur maximale de température **par mois** sur les trois années 2020, 2021 et 2022 dans la ville de Buffalo (données contenues dans les trois fichiers téléchargés précédemment).\n",
        "\n",
        "#### Question #10\n",
        "Tester le binaire exécutable `mapred streaming` en affichant son aide."
      ],
      "metadata": {
        "id": "-PtSdM2cqcN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "2ZVcaB_yrmFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #11\n",
        "Dans le répertoire courant, créer deux nouveaux fichiers `mapper.py` et `reducer.py` et ouvrir ces fichiers dans l'éditeur de Colab en double cliquant dessus.\n",
        "\n",
        "Vous insérerez la ligne suivante  au début de ces fichiers pour permettre leur exécution :\n",
        "\n",
        "`#!/usr/bin/env python`\n"
      ],
      "metadata": {
        "id": "_14mlf7DrnTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #12\n",
        "Ecrire un programme `mapper.py` permettant de découper chaque ligne récupérée sur l'entrée standard `stdin` (lignes des fichiers de données représentant des données météo journalières), et d'en extraire le mois et la température max journalière. La date est fournie en colonne 2 des fichiers et la température max en colonne 6.\n",
        "\n",
        "Le mois servira de clé intermédiaire et la température de valeur intermédiaire de sortie du `mapper`. Ces deux éléments seront imprimés sur la sortie standard séparés par une tabulation grâce à la fonction `print()`.\n",
        "\n",
        "L'entrée standard `stdin` pourra être récupérée grâce au module `sys`de Python."
      ],
      "metadata": {
        "id": "1p7uPTdsr_IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #13\n",
        "\n",
        "Ecrire un programme `reducer.py` qui récupère les clés (mois) et valeurs (température max journalière) intermédiaires (issues des processus `mapper`) sur l'entrée standard `stdin` et qui calcule le maximum de température par mois. Le mois et la température max par mois seront imprimés sur la sortie standard comme clé et valeur de sortie du processus MapReduce.\n",
        "\n",
        "*Indication : utiliser le fait que le `reducer` reçoive les paires de clé et valeur intermédiaires triés par clé.*"
      ],
      "metadata": {
        "id": "a1bIJNp3wpfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #14\n",
        "\n",
        "Vérifier que vous avez les droits d'exécution sur les fichiers `mapper.py` et `reducer.py`"
      ],
      "metadata": {
        "id": "1dP-sGf-aVgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "d8IZ_7Zoaj3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #15\n",
        "Pour tester votre programme `mapper.py` écrire une commande Unix permettant d'afficher le contenu des fichiers d'extension `.txt` (téléchargés dans le répertoir courant du système local) sur la sortie standard et de passer ce contenu sur l'entrée standard du programme `mapper.py`."
      ],
      "metadata": {
        "id": "qhjnS7xjaueW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "8TxqfK9MbS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #16\n",
        "Pour tester votre programme `reducer.py` écrire une commande Unix qui permet d'enchainer les traitements du `mapper` et du `reducer`.\n",
        "\n",
        "*Indication : on pourra utiliser la commande [`sort`](https://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html) de Unix.*"
      ],
      "metadata": {
        "id": "73g0fB8LbbmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "O_RKZPgjcFHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #17\n",
        "\n",
        "Ecrire la ligne de commande utilisant Hadoop Streaming permettant de lancer le traitement MapReduce sur Hadoop en utilisant les programmes `mapper.py` et `reducer.py`. Le résultat sera écrit dans le répertoire `/hdfs/output` du HDFS."
      ],
      "metadata": {
        "id": "Q08PabLscFwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "8QE1kOGvcqRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #18\n",
        "En utilisant Hadoop, visualiser le contenu du répertoire `/hdfs/output`"
      ],
      "metadata": {
        "id": "EMKjzkqkcrvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "C_RjIozAc3kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 19\n",
        "En utilisant Hadoop, afficher le contenu du fichier résultat du MapReduce."
      ],
      "metadata": {
        "id": "o_CJ5jDRc4XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "G4ZCrGn7dGRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question #20\n",
        "En utilisant Hadoop, transférer ce fichier résultat depuis le HDFS sur le système local (dans le répertoire courant)"
      ],
      "metadata": {
        "id": "0ovHti6-dIAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "9dCFPOn9dUYa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}