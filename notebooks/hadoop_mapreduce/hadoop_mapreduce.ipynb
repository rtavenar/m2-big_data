{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex4J40RXyKcG"
      },
      "source": [
        "# Map-Reduce en Hadoop\n",
        "\n",
        "Dans cette séance, vous allez implémenter des Map-Reduce en Hadoop.\n",
        "\n",
        "Pour cela, le principe de fonctionnement est simple :\n",
        "\n",
        "* un premier script Python (dit _mapper_) va lire l'entrée standard (`sys.stdin` en Python) ligne par ligne et, à chaque ligne lue, afficher le résultat de l'étape de _map_ (sur la sortie standard, via un `print()`);\n",
        "* ensuite Hadoop va centraliser toutes les paires clés-valeurs émises et les trier par ordre (alphabétique) croissant ;\n",
        "* enfin, un second script Python (dit _mapper_) va lire l'entrée standard (`sys.stdin` en Python) ligne par ligne (chaque ligne correspondant donc à une paire clé-valeur émise par le _mapper_, les lignes étant triées entre elles) et, pour chaque clé, afficher sur la sortie standard le résultat de l'opération de réduction.\n",
        "\n",
        "**Attention:** pour vous assurer que tous les programmes Python que vous écrirez dans ce TD puissent s'exécuter correctement, vous vous assurerez que :\n",
        "\n",
        "1. ils contiennent bien l'en-tête `#!/usr/bin/env python` en première ligne ;\n",
        "2. ils disposent bien des droits d'exécution pour l'utilisateur courant.\n",
        "\n",
        "## 1. Préambule : remis en route de `hadoop`\n",
        "\n",
        "Exécutez les cellules ci-dessous pour obtenir un environnement Hadoop fonctionnel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A09SleFM8jnk",
        "outputId": "e9e13ac3-e67c-40b1-eb3f-6be0fdcc4a0e"
      },
      "outputs": [],
      "source": [
        "# Télécharger et extraire hadoop\n",
        "\n",
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xzf hadoop-3.3.6.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH0rw8j_81II"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = os.popen(\"dirname $(dirname $(readlink -f $(which java)))\").read().strip()\n",
        "os.environ[\"PATH\"] += \":/content/hadoop-3.3.6/bin/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK1uF7DR0aus"
      },
      "source": [
        "**Question 1.1.** Créez un dossier `/hdfs/input` sur le HDFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coHYNrzv-p-R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnyfG8D_0o8V"
      },
      "source": [
        "**Question 1.2.** Téléchargez les données contenues dans l'archive <https://raw.githubusercontent.com/rtavenar/m2-big_data/main/notebooks/hadoop_mapreduce/hadoop_mapreduce_data.zip>, décompressez-les dans le dossier courant, puis copiez-les vers le dossier `/hdfs/input` du HDFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEuM2V9o1rmc"
      },
      "source": [
        "## 2. Premier exercice : calcul de maxima de température\n",
        "\n",
        "Le but de cette partie est d'utiliser MapReduce sur Hadoop pour obtenir la valeur maximale de température **par mois** sur les trois années 2020, 2021 et 2022 dans la ville de Buffalo.\n",
        "\n",
        "Les données correspondantes sont disponibles dans les fichiers `CRND0103-*.txt` extraits précédemment.\n",
        "\n",
        "**Question 2.1.** Ecrire un programme `mapper_Buffalo.py` permettant de découper chaque ligne récupérée sur l'entrée standard `stdin` (lignes des fichiers de données représentant des données météo journalières), et d'en extraire le mois et la température max journalière. La date est fournie en colonne 2 des fichiers et la température max en colonne 6.\n",
        "\n",
        "Le mois servira de clé intermédiaire et la température de valeur intermédiaire de sortie du `mapper`. Ces deux éléments seront imprimés sur la sortie standard séparés par une tabulation grâce à la fonction `print()`.\n",
        "\n",
        "L'entrée standard `stdin` pourra être récupérée grâce au module `sys`de Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdGRB6qx280B"
      },
      "source": [
        "**Question 2.2.** Pour tester votre programme `mapper_Buffalo.py`, écrire une commande Unix permettant d'afficher le contenu des fichiers de la forme `CRND0103-*.txt*.txt` (on travaille pour l'instant sur les fichiers locaux) sur la sortie standard et de passer ce contenu sur l'entrée standard du programme `mapper_Buffalo.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU_nR_-0vBf2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu3Fy_o74hCu"
      },
      "source": [
        "**Question 2.3.** Ecrire un programme `reducer_Buffalo.py` qui récupère les clés (mois) et valeurs (température max journalière) intermédiaires (issues des processus _mapper_) sur l'entrée standard `stdin` et qui calcule le maximum de température par mois. Le mois et la température max par mois seront imprimés sur la sortie standard comme clé et valeur de sortie du processus MapReduce.\n",
        "\n",
        "*Indication : utiliser le fait que le `reducer` reçoive les paires de clé et valeur intermédiaires triés par clé.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmoN3upZ4xwN"
      },
      "source": [
        "**Question 2.4.** Pour tester votre programme `reducer_Buffalo.py`, écrire une commande Unix qui permet d'enchainer les traitements du `mapper` et du `reducer`.\n",
        "\n",
        "*Indication : on pourra utiliser la commande [`sort`](https://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html) de Unix pour simuler le traitement intermédiaire de tri effectué par Hadoop.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrQREAdLvHCc",
        "outputId": "2abd1772-3be1-4293-8eec-51cd91a70f82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfedd6XI5cOU"
      },
      "source": [
        "**Question 2.5.** Utiliser `mapred streaming` pour enchaîner les opérations, cette fois-ci en utilisant Hadoop et les fichiers localisés sur le HDFS en entrée. Vous écrirez les sorties de votre traitement dans `/hdfs/output_Buffalo`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBaEho2QvtZK",
        "outputId": "aeac72c0-869e-481e-8e52-211fa8dc75da"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT_WGuJm5T6G"
      },
      "source": [
        "**Question 2.6.** Comment s'appelle le fichier dans lesquel a été écrit le résultat ? Affichez son contenu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-hB3vUEv8Dc",
        "outputId": "504b4fd3-22d3-43a1-8736-34b093161c24"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHr0cG66IYQ"
      },
      "source": [
        "## 3. Deuxième exercice : _wordcount_\n",
        "\n",
        "Dans cette partie, vous devrez implémenter un wordcount **en Hadoop MapReduce**.\n",
        "\n",
        "Pour cela vous devrez :\n",
        "\n",
        "* travailler sur les fichiers `lorem_*.txt` ;\n",
        "* implémenter un `mapper_wordcount.py` qui affichera, pour chaque mot lu, une sortie du type `le_mot_lu 1` ;\n",
        "* implémenter un `reducer_wordcount.py` qui fera la somme des comptes mot par mot ;\n",
        "* afficher le résultat pour vous assurer que tout a bien fonctionné."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejbP2Lt87NAn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv1sTI_W_QTQ"
      },
      "source": [
        "**Question subsidiaire.** Améliorer votre code (si besoin) pour que la ponctuation soit correctement prise en compte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdfzRJmO9L58"
      },
      "source": [
        "## Pour aller plus loin\n",
        "\n",
        "Faites de même en utilisant les fichiers `notes_*.csv` pour calculer les moyennes par étudiant (identifié par son numéro d'étudiant, première colonne).\n",
        "Vous considérerez que les fichiers `notes_*.csv` contiennent une note par ligne et que les moyennes sont à calculer sans tenir compte de quelconques coefficients et en mélangeant toutes les matières."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
